{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "w9_ANN_1101.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov0DLD9-O2_Z"
      },
      "source": [
        "# ANN\n",
        "- 2020-2 Artifitial Intelligence\n",
        "- week9 assignment\n",
        "- MNIST layer 변경 \n",
        "- updated 1101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UkzxcTl-XL2",
        "outputId": "4b7afb2f-a8d7-4ebb-8689-54bef947e5fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ogIulKo-kFz",
        "outputId": "da4fa7b1-5aa2-4f21-bf89-29e2e4ac32d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "cd /content/drive/My Drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwlC76Kp21Qz"
      },
      "source": [
        "# ready\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "root_dir = \"/AI_assignment\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEn_PRXdk5eh"
      },
      "source": [
        "<pre>\n",
        "<h2> step1) Load MNIST (X_train, y_train), (X_test, y_test)</h2>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7WN3AN_R6Q",
        "outputId": "05ab8328-3653-4034-9079-2b567436e86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "def load_dataset():\n",
        "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "  print(X_train.shape) #(60000, 28, 28)\n",
        "  print(y_train.shape)\n",
        "  print(X_test.shape) #(10000, 28, 28)\n",
        "  print(y_test.shape)\n",
        "  X_train = X_train.reshape(-1, 28*28)\n",
        "  \n",
        "  print(X_train.shape)\n",
        "  X_test  = X_test.reshape(-1, 28*28)\n",
        "\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float)\n",
        "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float)\n",
        "  y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "  return (X_train, y_train), (X_test, y_test)\n",
        "tmp = load_dataset()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qm_ZN-tOHbi"
      },
      "source": [
        "<pre>\n",
        "<h2> step2) NN Model MNIST</h2>\n",
        "layer_1, layer_3, layer_5, layer_10\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Yj5-JyNCLs"
      },
      "source": [
        "class MNIST(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(MNIST, self).__init__()\n",
        "\n",
        "    # config \n",
        "    self.width = config[\"input_width_size\"]\n",
        "    self.height = config[\"input_height_size\"]\n",
        "    self.feature_size = config[\"feature_size\"]\n",
        "    self.num_labels = config[\"num_labels\"]\n",
        "\n",
        "    self.layer_1 = nn.Sequential(\n",
        "        nn.Linear(in_features=self.width*self.height, out_features = self.num_labels),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.layer_3 = nn.Sequential(\n",
        "        nn.Linear(in_features=self.width*self.height, out_features = 256),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=256, out_features = 64),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=64, out_features = self.num_labels),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.layer_5 = nn.Sequential(\n",
        "        nn.Linear(in_features=self.width*self.height, out_features = 512),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=512, out_features = 256),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=256, out_features = 128),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=128, out_features = 64),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=64, out_features = self.num_labels),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.layer_10 = nn.Sequential(\n",
        "        nn.Linear(in_features=self.width*self.height, out_features = 600),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=600, out_features = 512),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=512, out_features = 400),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=400, out_features = 256),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=256, out_features = 128),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=128, out_features = 100),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=100, out_features = 64),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=64, out_features = 32),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=32, out_features = 16),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(in_features=16, out_features = self.num_labels),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, input_features, labels=None ):\n",
        "      input_features = self.layer_10(input_features)\n",
        "      \n",
        "      if labels is not None:\n",
        "           loss_fnc = nn.CrossEntropyLoss()\n",
        "           logits = input_features\n",
        "           loss = loss_fnc(logits, labels)\n",
        "           return loss\n",
        "           \n",
        "      else:\n",
        "          output = torch.argmax(input_features, -1)\n",
        "          return output\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Ngp0TbSwSK"
      },
      "source": [
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkKSHY2Hnln2"
      },
      "source": [
        "<pre>\n",
        "<h2> 2. 불러온 데이터를 이용하여 모델 학습 및 평가 </h2>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE5aJVyfRdn6"
      },
      "source": [
        "# test 데이터로 모델을 평가하는 함수\n",
        "def do_test(model, test_dataloader):\n",
        "  model.eval()\n",
        "  predicts, answers = [], []\n",
        "  for step, batch in enumerate(test_dataloader):\n",
        "    \n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "    input_features, labels = batch\n",
        "    output = model(input_features)\n",
        "\n",
        "    predicts.extend(tensor2list(output))\n",
        "    answers.extend(tensor2list(labels))\n",
        "    \n",
        "  print(\"Accuracy : {}\".format(accuracy_score(answers, predicts)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7fNFnA_Lhe"
      },
      "source": [
        "def train(config):\n",
        "  # 모델 생성\n",
        "  model = MNIST(config).cuda()\n",
        "\n",
        "  # 데이터 Load\n",
        "  (X_train, y_train), (X_test, y_test) = load_dataset()\n",
        "  \n",
        "  # TensorDataset/DataLoader를 통해 batch 단위로 데이터를 나누고 Shuffle\n",
        "  train_features = TensorDataset(X_train, y_train)\n",
        "  train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "  test_features = TensorDataset(X_test, y_test)\n",
        "  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  model.train()\n",
        "  for epoch in range(config[\"epoch\"]):\n",
        "    losses = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "      input_features, labels = batch\n",
        "\n",
        "      loss = model(input_features, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (step+1) % 1000 == 0:\n",
        "        print(\"{} step processed.. current loss : {}\".format(step+1, loss.data.item()))\n",
        "      losses.append(loss.data.item())\n",
        "    \n",
        "    print(\"Average Loss : {}\".format(np.mean(losses)))\n",
        "    # epoch이 끝날 때 마다, 모델 저장\n",
        "    torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
        "\n",
        "    # 지금까지 학습한 가중치로 평가 진행\n",
        "    do_test(model, test_dataloader)\n",
        "    \n",
        "\n",
        "def test(config):\n",
        "  model = MNIST(config).cuda()\n",
        "\n",
        "  # 저장된 모델 가중치 Load\n",
        "  model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "  # 데이터 load\n",
        "  (_, _), (X_test, y_test) = load_dataset()\n",
        "  \n",
        "  test_features = TensorDataset(X_test, y_test)\n",
        "  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "  \n",
        "  do_test(model, test_dataloader)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM4uBY2fMSM1"
      },
      "source": [
        "### step4) layer_1, layer_3, layer_5, layer_10 을 사용하여 hidden layer 를 늘려가면서 학습을 수행한 수 수행 결과를 비교해본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO3l1IKmL2ns"
      },
      "source": [
        "##### layer_1 : hidden layer 1개 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wbpqmro-xhg",
        "outputId": "6ba16212-96cb-4ef8-e9de-5d079581b557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"input_width_size\":28,\n",
        "              \"input_height_size\":28,\n",
        "              \"feature_size\": 512,\n",
        "              \"num_labels\": 10,\n",
        "              \"batch_size\":32,\n",
        "              \"epoch\":10,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n",
            "1000 step processed.. current loss : 1.620339274406433\n",
            "Average Loss : 1.7344207080841065\n",
            "Accuracy : 0.8155\n",
            "1000 step processed.. current loss : 1.6849464178085327\n",
            "Average Loss : 1.5999470479329427\n",
            "Accuracy : 0.8497\n",
            "1000 step processed.. current loss : 1.5320730209350586\n",
            "Average Loss : 1.5810541371663411\n",
            "Accuracy : 0.8846\n",
            "1000 step processed.. current loss : 1.4910818338394165\n",
            "Average Loss : 1.5723703810373941\n",
            "Accuracy : 0.8818\n",
            "1000 step processed.. current loss : 1.5283877849578857\n",
            "Average Loss : 1.5666840552012125\n",
            "Accuracy : 0.8936\n",
            "1000 step processed.. current loss : 1.6521902084350586\n",
            "Average Loss : 1.5632415227890015\n",
            "Accuracy : 0.8966\n",
            "1000 step processed.. current loss : 1.5530924797058105\n",
            "Average Loss : 1.5608470232009888\n",
            "Accuracy : 0.8847\n",
            "1000 step processed.. current loss : 1.60030996799469\n",
            "Average Loss : 1.5586708885828653\n",
            "Accuracy : 0.9027\n",
            "1000 step processed.. current loss : 1.4840083122253418\n",
            "Average Loss : 1.5569770146052042\n",
            "Accuracy : 0.9014\n",
            "1000 step processed.. current loss : 1.534298300743103\n",
            "Average Loss : 1.5550685330708822\n",
            "Accuracy : 0.8978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOY53kRXL99d"
      },
      "source": [
        "##### layer_3 : hidden layer 3개 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGiT1Qw_Qzo",
        "outputId": "53f07710-b373-46a7-e849-b624784f5e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"input_width_size\":28,\n",
        "              \"input_height_size\":28,\n",
        "              \"feature_size\": 512,\n",
        "              \"num_labels\": 10,\n",
        "              \"batch_size\":32,\n",
        "              \"epoch\":10,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n",
            "1000 step processed.. current loss : 1.946298360824585\n",
            "Average Loss : 1.98272950630188\n",
            "Accuracy : 0.8848\n",
            "1000 step processed.. current loss : 1.6984329223632812\n",
            "Average Loss : 1.7029244096755982\n",
            "Accuracy : 0.907\n",
            "1000 step processed.. current loss : 1.6096853017807007\n",
            "Average Loss : 1.6162887678146363\n",
            "Accuracy : 0.9242\n",
            "1000 step processed.. current loss : 1.5693327188491821\n",
            "Average Loss : 1.5743199388504028\n",
            "Accuracy : 0.9339\n",
            "1000 step processed.. current loss : 1.5609551668167114\n",
            "Average Loss : 1.5485924229303996\n",
            "Accuracy : 0.9405\n",
            "1000 step processed.. current loss : 1.4839526414871216\n",
            "Average Loss : 1.5323645364125569\n",
            "Accuracy : 0.9447\n",
            "1000 step processed.. current loss : 1.4872910976409912\n",
            "Average Loss : 1.5224092310587565\n",
            "Accuracy : 0.9472\n",
            "1000 step processed.. current loss : 1.5172085762023926\n",
            "Average Loss : 1.5151572317123414\n",
            "Accuracy : 0.9503\n",
            "1000 step processed.. current loss : 1.558019757270813\n",
            "Average Loss : 1.5096110370000204\n",
            "Accuracy : 0.954\n",
            "1000 step processed.. current loss : 1.4781957864761353\n",
            "Average Loss : 1.5056964487075806\n",
            "Accuracy : 0.9549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIwobi1kL_c0"
      },
      "source": [
        "##### layer_5 : hidden layer 5개 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UztI8PlMDFP",
        "outputId": "a977c93d-f0e4-4840-bdea-4fcd228888a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"input_width_size\":28,\n",
        "              \"input_height_size\":28,\n",
        "              \"feature_size\": 512,\n",
        "              \"num_labels\": 10,\n",
        "              \"batch_size\":32,\n",
        "              \"epoch\":10,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n",
            "1000 step processed.. current loss : 2.027773141860962\n",
            "Average Loss : 2.07231116104126\n",
            "Accuracy : 0.423\n",
            "1000 step processed.. current loss : 1.7940324544906616\n",
            "Average Loss : 1.8343861236572265\n",
            "Accuracy : 0.5169\n",
            "1000 step processed.. current loss : 1.7958078384399414\n",
            "Average Loss : 1.759981601079305\n",
            "Accuracy : 0.5304\n",
            "1000 step processed.. current loss : 1.7023024559020996\n",
            "Average Loss : 1.7212333463668823\n",
            "Accuracy : 0.5157\n",
            "1000 step processed.. current loss : 1.6659455299377441\n",
            "Average Loss : 1.7019661098480225\n",
            "Accuracy : 0.5508\n",
            "1000 step processed.. current loss : 1.6877518892288208\n",
            "Average Loss : 1.6918840938568116\n",
            "Accuracy : 0.4802\n",
            "1000 step processed.. current loss : 1.664292573928833\n",
            "Average Loss : 1.6854314500172933\n",
            "Accuracy : 0.4611\n",
            "1000 step processed.. current loss : 1.6671111583709717\n",
            "Average Loss : 1.679315783882141\n",
            "Accuracy : 0.4576\n",
            "1000 step processed.. current loss : 1.6657155752182007\n",
            "Average Loss : 1.6672776901245117\n",
            "Accuracy : 0.4615\n",
            "1000 step processed.. current loss : 1.615429401397705\n",
            "Average Loss : 1.6560781340281168\n",
            "Accuracy : 0.4645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2jK-uYRMGFD"
      },
      "source": [
        "##### layer_10 : hidden layer 10개 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zvYxDT6MH68",
        "outputId": "895b27e0-bbd0-4288-8317-756a222439b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"input_width_size\":28,\n",
        "              \"input_height_size\":28,\n",
        "              \"feature_size\": 512,\n",
        "              \"num_labels\": 10,\n",
        "              \"batch_size\":32,\n",
        "              \"epoch\":10,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n",
            "1000 step processed.. current loss : 2.292827606201172\n",
            "Average Loss : 2.3037627063751223\n",
            "Accuracy : 0.1135\n",
            "1000 step processed.. current loss : 2.232616901397705\n",
            "Average Loss : 2.2384571655273438\n",
            "Accuracy : 0.2078\n",
            "1000 step processed.. current loss : 2.0917508602142334\n",
            "Average Loss : 2.100410018793742\n",
            "Accuracy : 0.2106\n",
            "1000 step processed.. current loss : 2.073847532272339\n",
            "Average Loss : 2.0435284797668456\n",
            "Accuracy : 0.2113\n",
            "1000 step processed.. current loss : 1.9720739126205444\n",
            "Average Loss : 2.0209314589182537\n",
            "Accuracy : 0.211\n",
            "1000 step processed.. current loss : 2.02812123298645\n",
            "Average Loss : 2.0095047279993694\n",
            "Accuracy : 0.209\n",
            "1000 step processed.. current loss : 1.9392119646072388\n",
            "Average Loss : 2.0005126370747885\n",
            "Accuracy : 0.2106\n",
            "1000 step processed.. current loss : 1.9526029825210571\n",
            "Average Loss : 1.99466094900767\n",
            "Accuracy : 0.2101\n",
            "1000 step processed.. current loss : 1.9323465824127197\n",
            "Average Loss : 1.9919044684727987\n",
            "Accuracy : 0.2097\n",
            "1000 step processed.. current loss : 1.9429677724838257\n",
            "Average Loss : 1.989474365679423\n",
            "Accuracy : 0.2101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MftSvT25QLyV"
      },
      "source": [
        "# 결과\n",
        "|hidden_layer 갯수|loss|test_accuracy|\n",
        "|------|---|---|\n",
        "|layer_1|1.5550685330708822|0.8978|\n",
        "|layer_3|1.5056964487075806|0.9549|\n",
        "|layer_5|1.6560781340281168|0.4645|\n",
        "|layer_10|1.989474365679423|0.2101|\n",
        "\n",
        "accuracy가 가장 높을 때는 hidden layer를 3개 쌓을 때였다.  \n",
        "무조건 깊게 쌓는다고 해서 성능이 올라가지는 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCPPQUoKSJej"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}